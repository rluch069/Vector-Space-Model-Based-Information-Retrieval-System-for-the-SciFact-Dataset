<!DOCTYPE html>
<html><head></head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 12px;"><p>Assignment 1</p>
<p><strong>Information retrieval system [50 points]<o:p></o:p></strong></p>
<p><o:p>&nbsp;</o:p></p>
<p><strong>Note</strong>:&nbsp;<strong>The work should be done in groups of students and submitted via Brightspace. Once a group member submits, all group members can see the submission.<o:p></o:p></strong></p>
<p><o:p>&nbsp;</o:p></p>
<p>You will implement an Information Retrieval (IR) system based on the vector space model for a collection of documents. You will submit the results of your system on a set of test queries. You also have their ideal answers, the relevance judgments, so that you can evaluate the performance of your system before submission (for computing evaluation measures, you can use the <a href="https://trec.nist.gov/trec_eval/index.html">trec_eval</a>&nbsp;script, the latest version).</p>
<p><o:p>&nbsp;</o:p></p>
<p>We will use the&nbsp;<strong>Scifact dataset</strong>&nbsp;available&nbsp;<a href="https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip">here</a>. It is part of the BEIR collection.&nbsp;<a href="https://github.com/beir-cellar/beir/tree/main/beir">Here</a>&nbsp;is the link to the whole collection and associated code. Please use only the Scifact dataset (not any of the other datasets). The corpus/collection of the Scifact dataset consists of scientific claims and abstracts. Read more about the format of the collection, queries, and relevance judgements in&nbsp;<a href="https://arxiv.org/abs/2104.08663">this paper</a>. Use only test queries (the queries with odd numbers 1.3.5, …), not any training queries. Size of the dataset:</p>
<ul type="disc">
<li>corpus.jsonl: 7,917 KB</li>
<li>queries.jsonl: 205 KB</li>
<li>test.tsv: 6 KB</li>
</ul>
<p><o:p>&nbsp;</o:p></p>
<p>Implement an indexing scheme based on the vector space model, as discussed in class. Alternatively, you can use an existing IR system from the Internet and adapt it to work on this collection and queries (this system can use the vector space model or a more advanced model). The steps pointed out in class can be used as guidelines for the implementation. For weighting, you can use the tf-idf weighting scheme (w<sub>ij</sub>&nbsp;= tf<sub>ij</sub>∙idf<sub>i</sub>).&nbsp;<strong>Alternatively, you can use a similar formula for that is known to work well and lead to higher retrieval performance, called BM25.</strong>&nbsp;For each query, your system will produce a ranked list of documents, starting with the most similar to the query and ending with the least similar.</p>
<p><o:p>&nbsp;</o:p></p>
<p>Step1. [5 points]&nbsp;&nbsp;<strong>Preprocessing</strong>:&nbsp;&nbsp;Implement preprocessing functions for tokenization and stopword removal. The index terms will be all the words left after filtering out markup that is not part of the text, punctuation tokens, numbers, stopwords, etc. Optionally, you can use the Porter stemmer to stem the index words.&nbsp;</p>
<p>•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="text-decoration: underline;">Input</span>: Documents that are read one by one from the collection</p>
<p>•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="text-decoration: underline;">Output</span>: Tokens to be added to the index (vocabulary)</p>
<p>&nbsp;</p>
<p>Step 2. [10 points]&nbsp;<strong>Indexing</strong>: Build an inverted index, with an entry for each word in the vocabulary. You can use any appropriate data structure (hash table, linked lists, Access database, etc.). An example of a possible index is presented below. Note: If you use an existing IR system, use its indexing mechanism.</p>
<p>•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="text-decoration: underline;">Input</span>: Tokens obtained from the preprocessing module</p>
<p>•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="text-decoration: underline;">Output</span>: An inverted index for fast access</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>
<p>Step 3. [10 points]&nbsp;<strong>Retrieval and Ranking</strong>:&nbsp;&nbsp;Use the inverted index (from step 2) to find the limited set of documents that contain at least one of the query words. Compute the cosine similarity scores between a query and each document.&nbsp;</p>
<p><span style="text-decoration: underline;"><o:p>&nbsp;</o:p></span></p>
<p>•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="text-decoration: underline;">Input</span>: One query and the Inverted Index (from Step2)</p>
<p>•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="text-decoration: underline;">Output</span>: Similarity values between the query and each of the documents. Rank the documents in decreasing order of similarity scores.</p>
<p><o:p>&nbsp;</o:p></p>
<p>Run your system on the set of test queries.&nbsp;&nbsp;Include the output in your submission as a file named Results.&nbsp;&nbsp;[10 points]</p>
<p>The file should have the following format, for the top-100 results for each query/topic (the queries should be ordered in ascending order):</p>
<p>query_id Q0 doc_id rank score tag<br>where: query_id is the topic/query number, Q0 is an unused field (the literal 'Q0'), docno is&nbsp;&nbsp;the document id taken from the doc_id field of the segment, rank is the rank assigned by your system to the segment (1 is the highest rank), score is&nbsp;&nbsp;the computed degree of match between the segment and the topic, and tag is a unique identifier you chose for this run (same for every topic and segment). Example:<o:p></o:p></p>
<p>1 Q0 doc_id1 1 0.8032 run_name<o:p></o:p></p>
<p>1 Q0 doc_id2 0.7586 run_name<o:p></o:p></p>
<p>1 Q0 doc_id3 3 0.6517 run_name<o:p></o:p></p>
<p>…<o:p></o:p></p>
<p><o:p>&nbsp;</o:p></p>
<p>The relevance feedback file (expected solution) contains one or more relevant cdocuments for each query (any other documents are considered non-relevant). Example:</p>
<p>query-id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;doc-id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score</p>
<p>1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;31715818&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1</p>
<p>3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;14717500&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1</p>
<p>5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;13734012&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1</p>
<p>…</p>
<p><o:p>&nbsp;</o:p></p>
<p><strong>Resources</strong>: Here is a&nbsp;list of stopwords: <a href="List%20of%20Stopwords.html?isCourseFile=true" target="_blank" rel="noopener">List of Stopwords.html</a> you could use in Step 1. In Step1 you could use a regular expressions library to help with the filtering. If you want to use stemming, you can use the&nbsp;<a href="http://www.tartarus.org/~martin/PorterStemmer/">Porter stemmer</a>. Stemming might increase the performance a bit.</p>
<p><strong>One way to increase the performance is to add a pseudo-relevance feedback loop.<o:p></o:p></strong></p>
<p>&nbsp;</p>
<p><strong>Note</strong>: Feel free to add any optimizations or components that could improve the evaluation scores. Please explain them in your report and submit the Results file only for your best run. But&nbsp;<strong>please do not use neural information retrieval methods</strong>&nbsp;(based on deep learning, transformers, BERT or GPT models). We will use them in Assignment 2.</p>
<p><o:p>&nbsp;</o:p></p>
<p>Optionally, you can use some codes: <a href="IR_Files.zip?isCourseFile=true" target="_blank" rel="noopener">IR_Files.zip</a> from a former student who ran experiments on the BEIR collection (ignore the deep learning models for Assignment 1; you can use them in Assignment 2).</p>
<p>You can use any resources from the Internet, as long as you explain in your report how you used them (compilation, installation, adaptation, etc.). You can use any IR system available on the Internet (ElasticSearch, Lucene, Lemur, Terrier, Inquire, etc.).</p>
<p><o:p>&nbsp;</o:p></p>
<p><o:p>&nbsp;</o:p></p>
<p><strong>Submission instructions</strong>:</p>
<p>&nbsp;&nbsp;&nbsp;- write a README file (plain text, Word format, or pdf) [10 points for this&nbsp;<strong>report</strong>] including:</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* your names and student numbers. <strong>Specify how the tasks were divided between the team members [5 points] (if this info is not provided, the penalty is 5 points).&nbsp;<o:p></o:p></strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* a detailed note about the functionality of your programs,</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* complete instructions on how to run them</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* explain the algorithms, data structures, and optimizations that you used in each of the three steps. How big was the vocabulary? Include a sample of 100 tokens from your vocabulary. Include the first 10 answers to the first 2 queries. Discuss your results.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Include the Mean Average Precision (MAP) score computed with trec_eval for the results on the test queries.&nbsp;&nbsp;</p>
<p>&nbsp;&nbsp;&nbsp;- Produce a file named Results with the results for all the test queries for your best run, in the required format.</p>
<p>&nbsp;&nbsp;&nbsp;- for the queries/topic, use only the titles for one run and titles and full text for another run. Discuss which gives better results.</p>
<p>&nbsp;&nbsp;&nbsp;- Submit your assignment, including programs, README file, and Results file, as a zip file in Brightspace (only one team member needs to submit).</p>
<p>&nbsp;&nbsp;&nbsp;- don’t include the initial text collection.&nbsp;</p>
<p><o:p>&nbsp;</o:p></p>
<p><strong>Have fun :)</strong></p></body></html>